{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/anaconda3/envs/jb_py35/lib/python3.5/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['seed']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import time\n",
    "\n",
    "from keras import callbacks\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Input\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "\n",
    "#from scipy.misc import imread\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stop potential randomness\n",
    "seed = 128\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jbang36/eva/others/jupyter'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jbang36/eva/others/data/mnist'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = os.path.abspath('../')\n",
    "data_dir = os.path.join(root_dir, 'data', 'mnist')\n",
    "root_dir\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd741717ef0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image_index = 7777 # You may select anything up to 60,000\n",
    "print(train_y[image_index]) # The label is 8\n",
    "plt.imshow(train_x[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_jobs=-1, n_clusters=10, n_init=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape(-1, 784).astype('float32')\n",
    "test_x = test_x.reshape(-1, 784).astype('float32')\n",
    "train_x /= 255.0\n",
    "test_x /= 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time it took to train k_means is  150.07240200042725  seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "km.fit(train_x)\n",
    "print(\"Total time it took to train k_means is \", time.time() - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = km.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "kmeans_score = normalized_mutual_info_score(test_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(500, activation='relu')(input_img)\n",
    "encoded = Dense(500, activation='relu')(encoded)\n",
    "encoded = Dense(2000, activation='relu')(encoded)\n",
    "encoded = Dense(10, activation='sigmoid')(encoded)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(2000, activation='relu')(encoded)\n",
    "decoded = Dense(500, activation='relu')(decoded)\n",
    "decoded = Dense(500, activation='relu')(decoded)\n",
    "decoded = Dense(784)(decoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                20010     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2000)              22000     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 500)               1000500   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 784)               392784    \n",
      "=================================================================\n",
      "Total params: 3,330,794\n",
      "Trainable params: 3,330,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.0737 - val_loss: 0.0647\n",
      "Epoch 2/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0639 - val_loss: 0.0635\n",
      "Epoch 3/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0629 - val_loss: 0.0597\n",
      "Epoch 4/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0556 - val_loss: 0.0514\n",
      "Epoch 5/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0485 - val_loss: 0.0444\n",
      "Epoch 6/200\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0425 - val_loss: 0.0399\n",
      "Epoch 7/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0378 - val_loss: 0.0354\n",
      "Epoch 8/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0339 - val_loss: 0.0318\n",
      "Epoch 9/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0310 - val_loss: 0.0292\n",
      "Epoch 10/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0289 - val_loss: 0.0277\n",
      "Epoch 11/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0269 - val_loss: 0.0258\n",
      "Epoch 12/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0257 - val_loss: 0.0247\n",
      "Epoch 13/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0245 - val_loss: 0.0237\n",
      "Epoch 14/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0238 - val_loss: 0.0230\n",
      "Epoch 15/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0227 - val_loss: 0.0228\n",
      "Epoch 16/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0222 - val_loss: 0.0219\n",
      "Epoch 17/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0217 - val_loss: 0.0212\n",
      "Epoch 18/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0212 - val_loss: 0.0208\n",
      "Epoch 19/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0208 - val_loss: 0.0203\n",
      "Epoch 20/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0205 - val_loss: 0.0201\n",
      "Epoch 21/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0200 - val_loss: 0.0198\n",
      "Epoch 22/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0199 - val_loss: 0.0195\n",
      "Epoch 23/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0194 - val_loss: 0.0194\n",
      "Epoch 24/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0192 - val_loss: 0.0189\n",
      "Epoch 25/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0190 - val_loss: 0.0189\n",
      "Epoch 26/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0188 - val_loss: 0.0187\n",
      "Epoch 27/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0186 - val_loss: 0.0185\n",
      "Epoch 28/200\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0183 - val_loss: 0.0191\n",
      "Epoch 29/200\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0183 - val_loss: 0.0182\n",
      "Epoch 30/200\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0180 - val_loss: 0.0180\n",
      "Epoch 31/200\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0178 - val_loss: 0.0178\n",
      "Epoch 32/200\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0177 - val_loss: 0.0177\n",
      "Epoch 33/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0174 - val_loss: 0.0177\n",
      "Epoch 34/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0174 - val_loss: 0.0175\n",
      "Epoch 35/200\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0172 - val_loss: 0.0174\n",
      "Epoch 36/200\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0171 - val_loss: 0.0172\n",
      "Epoch 37/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0170 - val_loss: 0.0172\n",
      "Epoch 38/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 39/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 40/200\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 41/200\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0165 - val_loss: 0.0167\n",
      "Epoch 42/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0164 - val_loss: 0.0166\n",
      "Epoch 43/200\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0163 - val_loss: 0.0165\n",
      "Epoch 44/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0162 - val_loss: 0.0165\n",
      "Epoch 45/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0161 - val_loss: 0.0164\n",
      "Epoch 46/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0160 - val_loss: 0.0163\n",
      "Epoch 47/200\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0159 - val_loss: 0.0163\n",
      "Epoch 48/200\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0158 - val_loss: 0.0162\n",
      "Epoch 49/200\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0157 - val_loss: 0.0162\n",
      "Epoch 50/200\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0157 - val_loss: 0.0164\n",
      "Epoch 51/200\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0157 - val_loss: 0.0161\n",
      "Epoch 52/200\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0155 - val_loss: 0.0161\n",
      "Epoch 53/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0154 - val_loss: 0.0158\n",
      "Epoch 54/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0154 - val_loss: 0.0159\n",
      "Epoch 55/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0153 - val_loss: 0.0157\n",
      "Epoch 56/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0152 - val_loss: 0.0157\n",
      "Epoch 57/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0152 - val_loss: 0.0157\n",
      "Epoch 58/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0151 - val_loss: 0.0156\n",
      "Epoch 59/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0151 - val_loss: 0.0156\n",
      "Epoch 60/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0150 - val_loss: 0.0155\n",
      "Epoch 61/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0149 - val_loss: 0.0156\n",
      "Epoch 62/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0149 - val_loss: 0.0155\n",
      "Epoch 63/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0148 - val_loss: 0.0155\n",
      "Epoch 64/200\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0148 - val_loss: 0.0153\n",
      "Epoch 65/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0147 - val_loss: 0.0153\n",
      "Epoch 66/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0146 - val_loss: 0.0152\n",
      "Epoch 67/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0146 - val_loss: 0.0152\n",
      "Epoch 68/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0146 - val_loss: 0.0153\n",
      "Epoch 69/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0145 - val_loss: 0.0151\n",
      "Epoch 70/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0144 - val_loss: 0.0151\n",
      "Epoch 71/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0144 - val_loss: 0.0151\n",
      "Epoch 72/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0143 - val_loss: 0.0151\n",
      "Epoch 73/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0143 - val_loss: 0.0150\n",
      "Epoch 74/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0143 - val_loss: 0.0150\n",
      "Epoch 75/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0142 - val_loss: 0.0149\n",
      "Epoch 76/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0141 - val_loss: 0.0149\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0141 - val_loss: 0.0148\n",
      "Epoch 78/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0141 - val_loss: 0.0148\n",
      "Epoch 79/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0140 - val_loss: 0.0149\n",
      "Epoch 80/200\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0140 - val_loss: 0.0147\n",
      "Epoch 81/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0139 - val_loss: 0.0149\n",
      "Epoch 82/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0139 - val_loss: 0.0147\n",
      "Epoch 83/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0138 - val_loss: 0.0147\n",
      "Epoch 84/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0139 - val_loss: 0.0147\n",
      "Epoch 85/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0138 - val_loss: 0.0146\n",
      "Epoch 86/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0137 - val_loss: 0.0147\n",
      "Epoch 87/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0137 - val_loss: 0.0145\n",
      "Epoch 88/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0137 - val_loss: 0.0146\n",
      "Epoch 89/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0136 - val_loss: 0.0146\n",
      "Epoch 90/200\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0136 - val_loss: 0.0144\n",
      "Epoch 91/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0136 - val_loss: 0.0145\n",
      "Epoch 92/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0135 - val_loss: 0.0148\n",
      "Epoch 93/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0136 - val_loss: 0.0144\n",
      "Epoch 94/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0134 - val_loss: 0.0144\n",
      "Epoch 95/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0134 - val_loss: 0.0144\n",
      "Epoch 96/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0134 - val_loss: 0.0146\n",
      "Epoch 97/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0134 - val_loss: 0.0143\n",
      "Epoch 98/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0133 - val_loss: 0.0143\n",
      "Epoch 99/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0133 - val_loss: 0.0144\n",
      "Epoch 100/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0133 - val_loss: 0.0143\n",
      "Epoch 101/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0132 - val_loss: 0.0143\n",
      "Epoch 102/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0132 - val_loss: 0.0143\n",
      "Epoch 103/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0131 - val_loss: 0.0142\n",
      "Epoch 104/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0131 - val_loss: 0.0142\n",
      "Epoch 105/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0131 - val_loss: 0.0141\n",
      "Epoch 106/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0131 - val_loss: 0.0141\n",
      "Epoch 107/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0130 - val_loss: 0.0142\n",
      "Epoch 108/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0130 - val_loss: 0.0140\n",
      "Epoch 109/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0130 - val_loss: 0.0140\n",
      "Epoch 110/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0129 - val_loss: 0.0141\n",
      "Epoch 111/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0130 - val_loss: 0.0142\n",
      "Epoch 112/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0129 - val_loss: 0.0140\n",
      "Epoch 113/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0129 - val_loss: 0.0140\n",
      "Epoch 114/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0129 - val_loss: 0.0140\n",
      "Epoch 115/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0128 - val_loss: 0.0140\n",
      "Epoch 116/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0128 - val_loss: 0.0139\n",
      "Epoch 117/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0128 - val_loss: 0.0141\n",
      "Epoch 118/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0128 - val_loss: 0.0140\n",
      "Epoch 119/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0127 - val_loss: 0.0139\n",
      "Epoch 120/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0127 - val_loss: 0.0140\n",
      "Epoch 121/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0127 - val_loss: 0.0140\n",
      "Epoch 122/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0127 - val_loss: 0.0139\n",
      "Epoch 123/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0126 - val_loss: 0.0139\n",
      "Epoch 124/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0126 - val_loss: 0.0139\n",
      "Epoch 125/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0126 - val_loss: 0.0138\n",
      "Epoch 126/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0125 - val_loss: 0.0137\n",
      "Epoch 127/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0125 - val_loss: 0.0139\n",
      "Epoch 128/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0126 - val_loss: 0.0138\n",
      "Epoch 129/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0125 - val_loss: 0.0138\n",
      "Epoch 130/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0125 - val_loss: 0.0138\n",
      "Epoch 131/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0125 - val_loss: 0.0138\n",
      "Epoch 132/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0125 - val_loss: 0.0137\n",
      "Epoch 133/200\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0124 - val_loss: 0.0138\n",
      "Epoch 134/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0124 - val_loss: 0.0137\n",
      "Epoch 135/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0124 - val_loss: 0.0137\n",
      "Epoch 136/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0123 - val_loss: 0.0137\n",
      "Epoch 137/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0123 - val_loss: 0.0139\n",
      "Epoch 138/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0124 - val_loss: 0.0136\n",
      "Epoch 139/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0123 - val_loss: 0.0136\n",
      "Epoch 140/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0123 - val_loss: 0.0136\n",
      "Epoch 141/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0122 - val_loss: 0.0136\n",
      "Epoch 142/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0123 - val_loss: 0.0136\n",
      "Epoch 143/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0122 - val_loss: 0.0137\n",
      "Epoch 144/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0122 - val_loss: 0.0136\n",
      "Epoch 145/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0122 - val_loss: 0.0137\n",
      "Epoch 146/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0122 - val_loss: 0.0135\n",
      "Epoch 147/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0122 - val_loss: 0.0137\n",
      "Epoch 148/200\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0121 - val_loss: 0.0136\n",
      "Epoch 149/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0121 - val_loss: 0.0136\n",
      "Epoch 150/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0121 - val_loss: 0.0136\n",
      "Epoch 151/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0121 - val_loss: 0.0135\n",
      "Epoch 152/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0120 - val_loss: 0.0134\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0121 - val_loss: 0.0135\n",
      "Epoch 154/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0120 - val_loss: 0.0134\n",
      "Epoch 155/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0120 - val_loss: 0.0136\n",
      "Epoch 156/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0120 - val_loss: 0.0134\n",
      "Epoch 157/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0120 - val_loss: 0.0134\n",
      "Epoch 158/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0119 - val_loss: 0.0135\n",
      "Epoch 159/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0119 - val_loss: 0.0134\n",
      "Epoch 160/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0120 - val_loss: 0.0134\n",
      "Epoch 161/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0119 - val_loss: 0.0134\n",
      "Epoch 162/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0119 - val_loss: 0.0134\n",
      "Epoch 163/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0119 - val_loss: 0.0133\n",
      "Epoch 164/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0119 - val_loss: 0.0135\n",
      "Epoch 165/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0119 - val_loss: 0.0133\n",
      "Epoch 166/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0118 - val_loss: 0.0134\n",
      "Epoch 167/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0119 - val_loss: 0.0133\n",
      "Epoch 168/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0118 - val_loss: 0.0133\n",
      "Epoch 169/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0118 - val_loss: 0.0133\n",
      "Epoch 170/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0118 - val_loss: 0.0133\n",
      "Epoch 171/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0117 - val_loss: 0.0133\n",
      "Epoch 172/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0117 - val_loss: 0.0134\n",
      "Epoch 173/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0118 - val_loss: 0.0134\n",
      "Epoch 174/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0118 - val_loss: 0.0133\n",
      "Epoch 175/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0117 - val_loss: 0.0133\n",
      "Epoch 176/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0117 - val_loss: 0.0132\n",
      "Epoch 177/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0116 - val_loss: 0.0133\n",
      "Epoch 178/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0117 - val_loss: 0.0132\n",
      "Epoch 179/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0116 - val_loss: 0.0133\n",
      "Epoch 180/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0116 - val_loss: 0.0132\n",
      "Epoch 181/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0116 - val_loss: 0.0133\n",
      "Epoch 182/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0116 - val_loss: 0.0132\n",
      "Epoch 183/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0116 - val_loss: 0.0132\n",
      "Epoch 184/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0116 - val_loss: 0.0132\n",
      "Epoch 185/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0116 - val_loss: 0.0132\n",
      "Epoch 186/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0115 - val_loss: 0.0132\n",
      "Epoch 187/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0115 - val_loss: 0.0131\n",
      "Epoch 188/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0115 - val_loss: 0.0131\n",
      "Epoch 189/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0116 - val_loss: 0.0131\n",
      "Epoch 190/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0115 - val_loss: 0.0133\n",
      "Epoch 191/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0116 - val_loss: 0.0131\n",
      "Epoch 192/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0114 - val_loss: 0.0131\n",
      "Epoch 193/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0115 - val_loss: 0.0132\n",
      "Epoch 194/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0115 - val_loss: 0.0131\n",
      "Epoch 195/200\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0114 - val_loss: 0.0132\n",
      "Epoch 196/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0114 - val_loss: 0.0131\n",
      "Epoch 197/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0115 - val_loss: 0.0132\n",
      "Epoch 198/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0114 - val_loss: 0.0131\n",
      "Epoch 199/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0114 - val_loss: 0.0131\n",
      "Epoch 200/200\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0113 - val_loss: 0.0132\n",
      "Total time it took to train autoencoder is  121.46980023384094  seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_history = autoencoder.fit(train_x, train_x, epochs=200, batch_size=2048, validation_data=(test_x, test_x))\n",
    "print(\"Total time it took to train autoencoder is \", time.time() - start_time, \" seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_auto_train = encoder.predict(train_x)\n",
    "pred_auto = encoder.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to fit  60000 :  186.00084447860718\n",
      "0\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of datapoints')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/anaconda3/envs/jb_py35/lib/python3.5/site-packages/matplotlib/figure.py:2369: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  warnings.warn(\"This figure includes Axes that are not compatible \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters=10)\n",
    "\n",
    "start_time = time.time()\n",
    "ac.fit(pred_auto_train)\n",
    "n_samples = pred_auto_train.shape[0]\n",
    "print(\"Time to fit \", n_samples, \": \", time.time() - start_time)\n",
    "images_label = ac.labels_\n",
    "\n",
    "print(min(images_label))\n",
    "print(max(images_label))\n",
    "\n",
    "# I think it is best to see the distributions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "axs.hist(images_label, bins=10)\n",
    "plt.xlabel(\"Clusters\")\n",
    "plt.ylabel(\"Number of datapoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans \n",
    "km_auto = KMeans(n_jobs=-1, n_clusters=10, n_init=20)\n",
    "start_time = time.time()\n",
    "km_auto.fit(pred_auto_train)\n",
    "print(\"Total time it took to train k_means is \", time.time() - start_time, \" seconds\")\n",
    "\n",
    "pred = km_auto.predict(pred_auto)\n",
    "auto_score = normalized_mutual_info_score(test_y, pred)\n",
    "auto_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density clustering\n",
    "\n",
    "import numpy as numpy\n",
    "import scipy as scipy\n",
    "from sklearn import cluster\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Util functions\n",
    "def set2List(NumpyArray):\n",
    "    list = []\n",
    "    for item in NumpyArray:\n",
    "        list.append(item.tolist())\n",
    "    return list\n",
    " \n",
    "def GenerateData():\n",
    "    x1=numpy.random.randn(50,2)\n",
    "    x2x=numpy.random.randn(80,1)+12\n",
    "    x2y=numpy.random.randn(80,1)\n",
    "    x2=numpy.column_stack((x2x,x2y))\n",
    "    x3=numpy.random.randn(100,2)+8\n",
    "    x4=numpy.random.randn(120,2)+15\n",
    "    z=numpy.concatenate((x1,x2,x3,x4))\n",
    "    return z\n",
    "\n",
    "#DBSCAN functions\n",
    "def DBSCAN(Dataset, Epsilon,MinumumPoints,DistanceMethod = 'euclidean'):\n",
    "#    Dataset is a mxn matrix, m is number of item and n is the dimension of data\n",
    "    m,n=Dataset.shape\n",
    "    Visited=numpy.zeros(m,'int')\n",
    "    Type=numpy.zeros(m)\n",
    "#   -1 noise, outlier\n",
    "#    0 border\n",
    "#    1 core\n",
    "    ClustersList=[]\n",
    "    Cluster=[]\n",
    "    PointClusterNumber=numpy.zeros(m)\n",
    "    PointClusterNumberIndex=1\n",
    "    PointNeighbors=[]\n",
    "    DistanceMatrix = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(Dataset, DistanceMethod))\n",
    "    for i in range(m):\n",
    "        if Visited[i]==0:\n",
    "            Visited[i]=1\n",
    "            PointNeighbors=numpy.where(DistanceMatrix[i]<Epsilon)[0]\n",
    "            if len(PointNeighbors)<MinumumPoints:\n",
    "                Type[i]=-1\n",
    "            else:\n",
    "                for k in range(len(Cluster)):\n",
    "                    Cluster.pop()\n",
    "                Cluster.append(i)\n",
    "                PointClusterNumber[i]=PointClusterNumberIndex\n",
    "                \n",
    "                \n",
    "                PointNeighbors=set2List(PointNeighbors)    \n",
    "                ExpandClsuter(Dataset[i], PointNeighbors,Cluster,MinumumPoints,Epsilon,Visited,DistanceMatrix,PointClusterNumber,PointClusterNumberIndex  )\n",
    "                Cluster.append(PointNeighbors[:])\n",
    "                ClustersList.append(Cluster[:])\n",
    "                PointClusterNumberIndex=PointClusterNumberIndex+1\n",
    "                 \n",
    "                    \n",
    "    return PointClusterNumber \n",
    " \n",
    " \n",
    " \n",
    "def ExpandClsuter(PointToExapnd, PointNeighbors,Cluster,MinumumPoints,Epsilon,Visited,DistanceMatrix,PointClusterNumber,PointClusterNumberIndex  ):\n",
    "    Neighbors=[]\n",
    " \n",
    "    for i in PointNeighbors:\n",
    "        if Visited[i]==0:\n",
    "            Visited[i]=1\n",
    "            Neighbors=numpy.where(DistanceMatrix[i]<Epsilon)[0]\n",
    "            if len(Neighbors)>=MinumumPoints:\n",
    "#                Neighbors merge with PointNeighbors\n",
    "                for j in Neighbors:\n",
    "                    try:\n",
    "                        PointNeighbors.index(j)\n",
    "                    except ValueError:\n",
    "                        PointNeighbors.append(j)\n",
    "                    \n",
    "        if PointClusterNumber[i]==0:\n",
    "            Cluster.append(i)\n",
    "            PointClusterNumber[i]=PointClusterNumberIndex\n",
    "    return\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(pred_auto_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-d4965d20bfeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_auto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mDBSCAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEpsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMinumumPoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total time it took to for density clustering evaluation is \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-7cf45ff6ca9c>\u001b[0m in \u001b[0;36mDBSCAN\u001b[0;34m(Dataset, Epsilon, MinumumPoints, DistanceMethod)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mPointNeighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset2List\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPointNeighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mExpandClsuter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPointNeighbors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCluster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMinumumPoints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEpsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVisited\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDistanceMatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPointClusterNumber\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPointClusterNumberIndex\u001b[0m  \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0mCluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPointNeighbors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mClustersList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCluster\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-7cf45ff6ca9c>\u001b[0m in \u001b[0;36mExpandClsuter\u001b[0;34m(PointToExapnd, PointNeighbors, Cluster, MinumumPoints, Epsilon, Visited, DistanceMatrix, PointClusterNumber, PointClusterNumberIndex)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mNeighbors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                         \u001b[0mPointNeighbors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                         \u001b[0mPointNeighbors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Density Clustering Parameters\n",
    "\n",
    "Epsilon = 1 #euclidean distance\n",
    "MinumumPoints = 500\n",
    "Data = pred_auto\n",
    "start_time = time.time()\n",
    "result =DBSCAN(Data,Epsilon,MinumumPoints)\n",
    "print(\"Total time it took to for density clustering evaluation is \", time.time() - start_time, \" seconds\")\n",
    "\n",
    "max_result = max(result)\n",
    "print(max_result)\n",
    "for i in range(max_result):    \n",
    "    print(sum(result == i))\n",
    "print(sum(result == max_result))\n",
    "auto_score = normalized_mutual_info_score(test_y, result)\n",
    "auto_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Pretraining...\n",
      "Epoch 1/500\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.0861\n",
      "        |==>  acc: 0.0935,  nmi: 0.0989  <==|\n",
      "Epoch 2/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0653"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0642\n",
      "Epoch 3/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0635\n",
      "Epoch 4/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0633\n",
      "Epoch 5/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0625\n",
      "Epoch 6/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0567\n",
      "Epoch 7/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0534\n",
      "Epoch 8/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0482\n",
      "Epoch 9/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0415\n",
      "Epoch 10/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0355\n",
      "Epoch 11/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0315\n",
      "Epoch 12/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0293\n",
      "Epoch 13/500\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0278\n",
      "Epoch 14/500\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0260\n",
      "Epoch 15/500\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0245\n",
      "Epoch 16/500\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0233\n",
      "Epoch 17/500\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.0221\n",
      "Epoch 18/500\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0215\n",
      "Epoch 19/500\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.0205\n",
      "Epoch 20/500\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.0199\n",
      "Epoch 21/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0194\n",
      "Epoch 22/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0189\n",
      "Epoch 23/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0184\n",
      "Epoch 24/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0182\n",
      "Epoch 25/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0178\n",
      "Epoch 26/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0175\n",
      "Epoch 27/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0173\n",
      "Epoch 28/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0171\n",
      "Epoch 29/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0169\n",
      "Epoch 30/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0166\n",
      "Epoch 31/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0165\n",
      "Epoch 32/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0163\n",
      "Epoch 33/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0161\n",
      "Epoch 34/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0160\n",
      "Epoch 35/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0159\n",
      "Epoch 36/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0157\n",
      "Epoch 37/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0157\n",
      "Epoch 38/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0154\n",
      "Epoch 39/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0154\n",
      "Epoch 40/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0152\n",
      "Epoch 41/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0151\n",
      "Epoch 42/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0150\n",
      "Epoch 43/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0150\n",
      "Epoch 44/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0149\n",
      "Epoch 45/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0147\n",
      "Epoch 46/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0146\n",
      "Epoch 47/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0146\n",
      "Epoch 48/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0145\n",
      "Epoch 49/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0144\n",
      "Epoch 50/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0143\n",
      "Epoch 51/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0143\n",
      "        |==>  acc: 0.0196,  nmi: 0.6981  <==|\n",
      "Epoch 52/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0144"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0142\n",
      "Epoch 53/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0141\n",
      "Epoch 54/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0140\n",
      "Epoch 55/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0139\n",
      "Epoch 56/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0139\n",
      "Epoch 57/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0138\n",
      "Epoch 58/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0138\n",
      "Epoch 59/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0137\n",
      "Epoch 60/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0137\n",
      "Epoch 61/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0135\n",
      "Epoch 62/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0136\n",
      "Epoch 63/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0134\n",
      "Epoch 64/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0134\n",
      "Epoch 65/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0134\n",
      "Epoch 66/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0133\n",
      "Epoch 67/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0132\n",
      "Epoch 68/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0132\n",
      "Epoch 69/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0132\n",
      "Epoch 70/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0131\n",
      "Epoch 71/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0130\n",
      "Epoch 72/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0130\n",
      "Epoch 73/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0129\n",
      "Epoch 74/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0129\n",
      "Epoch 75/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0129\n",
      "Epoch 76/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0128\n",
      "Epoch 77/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0128\n",
      "Epoch 78/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0127\n",
      "Epoch 79/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0127\n",
      "Epoch 80/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0127\n",
      "Epoch 81/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0126\n",
      "Epoch 82/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0125\n",
      "Epoch 83/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0126\n",
      "Epoch 84/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0125\n",
      "Epoch 85/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0124\n",
      "Epoch 86/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0125\n",
      "Epoch 87/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0123\n",
      "Epoch 88/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0123\n",
      "Epoch 89/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0123\n",
      "Epoch 90/500\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0123\n",
      "Epoch 91/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0122\n",
      "Epoch 92/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0122\n",
      "Epoch 93/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0122\n",
      "Epoch 94/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0121\n",
      "Epoch 95/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0121\n",
      "Epoch 96/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0121\n",
      "Epoch 97/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0120\n",
      "Epoch 98/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0120\n",
      "Epoch 99/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0120\n",
      "Epoch 100/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0120\n",
      "Epoch 101/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0120\n",
      "        |==>  acc: 0.1176,  nmi: 0.7413  <==|\n",
      "Epoch 102/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0118"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0118\n",
      "Epoch 103/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0119\n",
      "Epoch 104/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0118\n",
      "Epoch 105/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0119\n",
      "Epoch 106/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0118\n",
      "Epoch 107/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0117\n",
      "Epoch 108/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0117\n",
      "Epoch 109/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0117\n",
      "Epoch 110/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0116\n",
      "Epoch 111/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0116\n",
      "Epoch 112/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0116\n",
      "Epoch 113/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0116\n",
      "Epoch 114/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0115\n",
      "Epoch 115/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0115\n",
      "Epoch 116/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0115\n",
      "Epoch 117/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0114\n",
      "Epoch 118/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0114\n",
      "Epoch 119/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0115\n",
      "Epoch 120/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0114\n",
      "Epoch 121/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0113\n",
      "Epoch 122/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0113\n",
      "Epoch 123/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0114\n",
      "Epoch 124/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0113\n",
      "Epoch 125/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0112\n",
      "Epoch 126/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0113\n",
      "Epoch 127/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0113\n",
      "Epoch 128/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0112\n",
      "Epoch 129/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0111\n",
      "Epoch 130/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0112\n",
      "Epoch 131/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0111\n",
      "Epoch 132/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0112\n",
      "Epoch 133/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0111\n",
      "Epoch 134/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0111\n",
      "Epoch 135/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0110\n",
      "Epoch 136/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0110\n",
      "Epoch 137/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0110\n",
      "Epoch 138/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0110\n",
      "Epoch 139/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0110\n",
      "Epoch 140/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0109\n",
      "Epoch 141/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0110\n",
      "Epoch 142/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0109\n",
      "Epoch 143/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0109\n",
      "Epoch 144/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0109\n",
      "Epoch 145/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0108\n",
      "Epoch 146/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0108\n",
      "Epoch 147/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0109\n",
      "Epoch 148/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0108\n",
      "Epoch 149/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0108\n",
      "Epoch 150/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0108\n",
      "Epoch 151/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0107\n",
      "        |==>  acc: 0.0411,  nmi: 0.7550  <==|\n",
      "Epoch 152/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0106"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0107\n",
      "Epoch 153/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0108\n",
      "Epoch 154/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0107\n",
      "Epoch 155/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0107\n",
      "Epoch 156/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0107\n",
      "Epoch 157/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0107\n",
      "Epoch 158/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0106\n",
      "Epoch 159/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0107\n",
      "Epoch 160/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0106\n",
      "Epoch 161/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0106\n",
      "Epoch 162/500\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0106\n",
      "Epoch 163/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0106\n",
      "Epoch 164/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0105\n",
      "Epoch 165/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0105\n",
      "Epoch 166/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0106\n",
      "Epoch 167/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0105\n",
      "Epoch 168/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0104\n",
      "Epoch 169/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0105\n",
      "Epoch 170/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0105\n",
      "Epoch 171/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0105\n",
      "Epoch 172/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0105\n",
      "Epoch 173/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0104\n",
      "Epoch 174/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103\n",
      "Epoch 175/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0104\n",
      "Epoch 176/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0104\n",
      "Epoch 177/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103\n",
      "Epoch 178/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0103\n",
      "Epoch 179/500\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0103\n",
      "Epoch 180/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0103\n",
      "Epoch 181/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103\n",
      "Epoch 182/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103\n",
      "Epoch 183/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0102\n",
      "Epoch 184/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0102\n",
      "Epoch 185/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0102\n",
      "Epoch 186/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0102\n",
      "Epoch 187/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0103\n",
      "Epoch 188/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0102\n",
      "Epoch 189/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0102\n",
      "Epoch 190/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0102\n",
      "Epoch 191/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0102\n",
      "Epoch 192/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0102\n",
      "Epoch 193/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0102\n",
      "Epoch 194/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0101\n",
      "Epoch 195/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0101\n",
      "Epoch 196/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0100\n",
      "Epoch 197/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0101\n",
      "Epoch 198/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0101\n",
      "Epoch 199/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0101\n",
      "Epoch 200/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0101\n",
      "Epoch 201/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0101\n",
      "        |==>  acc: 0.0119,  nmi: 0.7606  <==|\n",
      "Epoch 202/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0099"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0100\n",
      "Epoch 203/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0100\n",
      "Epoch 204/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0100\n",
      "Epoch 205/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0101\n",
      "Epoch 206/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0100\n",
      "Epoch 207/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0100\n",
      "Epoch 208/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0100\n",
      "Epoch 209/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0099\n",
      "Epoch 210/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0100\n",
      "Epoch 211/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0099\n",
      "Epoch 212/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0099\n",
      "Epoch 213/500\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0099\n",
      "Epoch 214/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0099\n",
      "Epoch 215/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0098\n",
      "Epoch 216/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0099\n",
      "Epoch 217/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0099\n",
      "Epoch 218/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0098\n",
      "Epoch 219/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0099\n",
      "Epoch 220/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0098\n",
      "Epoch 221/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0098\n",
      "Epoch 222/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0098\n",
      "Epoch 223/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0098\n",
      "Epoch 224/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0098\n",
      "Epoch 225/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0099\n",
      "Epoch 226/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0097\n",
      "Epoch 227/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0097\n",
      "Epoch 228/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0098\n",
      "Epoch 229/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0097\n",
      "Epoch 230/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0097\n",
      "Epoch 231/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0097\n",
      "Epoch 232/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0097\n",
      "Epoch 233/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0098\n",
      "Epoch 234/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0097\n",
      "Epoch 235/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0096\n",
      "Epoch 236/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0097\n",
      "Epoch 237/500\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0097\n",
      "Epoch 238/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0096\n",
      "Epoch 239/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0097\n",
      "Epoch 240/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0097\n",
      "Epoch 241/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0097\n",
      "Epoch 242/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0096\n",
      "Epoch 243/500\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0096\n",
      "Epoch 244/500\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0096\n",
      "Epoch 245/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0096\n",
      "Epoch 246/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0096\n",
      "Epoch 247/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0096\n",
      "Epoch 248/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0095\n",
      "Epoch 249/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0095\n",
      "Epoch 250/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0096\n",
      "Epoch 251/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0096\n",
      "        |==>  acc: 0.1732,  nmi: 0.7648  <==|\n",
      "Epoch 252/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0096"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0095\n",
      "Epoch 253/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0095\n",
      "Epoch 254/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0095\n",
      "Epoch 255/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0096\n",
      "Epoch 256/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0095\n",
      "Epoch 257/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0095\n",
      "Epoch 258/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 259/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0095\n",
      "Epoch 260/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0095\n",
      "Epoch 261/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 262/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 263/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0095\n",
      "Epoch 264/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 265/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 266/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 267/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 268/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0094\n",
      "Epoch 269/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0095\n",
      "Epoch 270/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 271/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 272/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 273/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 274/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 275/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 276/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 277/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 278/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 279/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0094\n",
      "Epoch 280/500\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0093\n",
      "Epoch 281/500\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0093\n",
      "Epoch 282/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 283/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 284/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 285/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0092\n",
      "Epoch 286/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 287/500\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0093\n",
      "Epoch 288/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0093\n",
      "Epoch 289/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0092\n",
      "Epoch 290/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0092\n",
      "Epoch 291/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 292/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0092\n",
      "Epoch 293/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0092\n",
      "Epoch 294/500\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.0093\n",
      "Epoch 295/500\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0092\n",
      "Epoch 296/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 297/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 298/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0093\n",
      "Epoch 299/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0092\n",
      "Epoch 300/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0092\n",
      "Epoch 301/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0092\n",
      "        |==>  acc: 0.0069,  nmi: 0.7724  <==|\n",
      "Epoch 302/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0093"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0092\n",
      "Epoch 303/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0092\n",
      "Epoch 304/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 305/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 306/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0092\n",
      "Epoch 307/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 308/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0091\n",
      "Epoch 309/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0092\n",
      "Epoch 310/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 311/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 312/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 313/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 314/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 315/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 316/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 317/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0092\n",
      "Epoch 318/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 319/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 320/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0090\n",
      "Epoch 321/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 322/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 323/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 324/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 325/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0090\n",
      "Epoch 326/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0091\n",
      "Epoch 327/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 328/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 329/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0090\n",
      "Epoch 330/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0090\n",
      "Epoch 331/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0090\n",
      "Epoch 332/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0090\n",
      "Epoch 333/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 334/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 335/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 336/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 337/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0089\n",
      "Epoch 338/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 339/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0089\n",
      "Epoch 340/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 341/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0089\n",
      "Epoch 342/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 343/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0089\n",
      "Epoch 344/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 345/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0090\n",
      "Epoch 346/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0089\n",
      "Epoch 347/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0089\n",
      "Epoch 348/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0089\n",
      "Epoch 349/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0089\n",
      "Epoch 350/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0089\n",
      "Epoch 351/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0089\n",
      "        |==>  acc: 0.0985,  nmi: 0.7671  <==|\n",
      "Epoch 352/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0090"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0089\n",
      "Epoch 353/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0089\n",
      "Epoch 354/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0088\n",
      "Epoch 355/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0089\n",
      "Epoch 356/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 357/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0089\n",
      "Epoch 358/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0089\n",
      "Epoch 359/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0088\n",
      "Epoch 360/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 361/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 362/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 363/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0088\n",
      "Epoch 364/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0088\n",
      "Epoch 365/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0088\n",
      "Epoch 366/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0088\n",
      "Epoch 367/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 368/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 369/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 370/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 371/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 372/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 373/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 374/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 375/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 376/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0088\n",
      "Epoch 377/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0088\n",
      "Epoch 378/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 379/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 380/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 381/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 382/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 383/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 384/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 385/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 386/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 387/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 388/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 389/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 390/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 391/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 392/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 393/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 394/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 395/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 396/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 397/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 398/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 399/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 400/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 401/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "        |==>  acc: 0.1019,  nmi: 0.7714  <==|\n",
      "Epoch 402/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0086"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0086\n",
      "Epoch 403/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 404/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 405/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 406/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 407/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 408/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 409/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 410/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 411/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0086\n",
      "Epoch 412/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 413/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0086\n",
      "Epoch 414/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 415/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 416/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 417/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 418/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 419/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 420/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 421/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 422/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 423/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 424/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 425/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 426/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0085\n",
      "Epoch 427/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0086\n",
      "Epoch 428/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 429/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 430/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 431/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0085\n",
      "Epoch 432/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0085\n",
      "Epoch 433/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 434/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 435/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 436/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 437/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 438/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 439/500\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0084\n",
      "Epoch 440/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0085\n",
      "Epoch 441/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 442/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 443/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0085\n",
      "Epoch 444/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0084\n",
      "Epoch 445/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0085\n",
      "Epoch 446/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0084\n",
      "Epoch 447/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 448/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 449/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 450/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 451/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "        |==>  acc: 0.0612,  nmi: 0.7731  <==|\n",
      "Epoch 452/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0086"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0085\n",
      "Epoch 453/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0084\n",
      "Epoch 454/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0084\n",
      "Epoch 455/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0085\n",
      "Epoch 456/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 457/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 458/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 459/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 460/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 461/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 462/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 463/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 464/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0084\n",
      "Epoch 465/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 466/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0084\n",
      "Epoch 467/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0084\n",
      "Epoch 468/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0084\n",
      "Epoch 469/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 470/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 471/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 472/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 473/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 474/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 475/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 476/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 477/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 478/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 479/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 480/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0083\n",
      "Epoch 481/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 482/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0083\n",
      "Epoch 483/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0084\n",
      "Epoch 484/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 485/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 486/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 487/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 488/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0084\n",
      "Epoch 489/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 490/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 491/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 492/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0083\n",
      "Epoch 493/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0083\n",
      "Epoch 494/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0083\n",
      "Epoch 495/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0083\n",
      "Epoch 496/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 497/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 498/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0082\n",
      "Epoch 499/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0083\n",
      "Epoch 500/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0083\n",
      "Pretraining time:  877.1099791526794\n",
      "Pretrained weights are saved to results/ae_weights.h5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Keras implementation for Deep Embedded Clustering (DEC) algorithm:\n",
    "\n",
    "Original Author:\n",
    "    Xifeng Guo. 2017.1.30\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "    \"\"\"\n",
    "    Fully connected auto-encoder model, symmetric.\n",
    "    Arguments:\n",
    "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
    "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
    "        act: activation, not applied to Input, Hidden and Output layers\n",
    "    return:\n",
    "        (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "    # input\n",
    "    x = Input(shape=(dims[0],), name='input')\n",
    "    h = x\n",
    "\n",
    "    # internal layers in encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)\n",
    "\n",
    "    # hidden layer\n",
    "    h = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here\n",
    "\n",
    "    y = h\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        y = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(y)\n",
    "\n",
    "    # output\n",
    "    y = Dense(dims[0], kernel_initializer=init, name='decoder_0')(y)\n",
    "\n",
    "    return Model(inputs=x, outputs=y, name='AE'), Model(inputs=x, outputs=h, name='encoder')\n",
    "\n",
    "\n",
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight((self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class DEC(object):\n",
    "    def __init__(self,\n",
    "                 dims,\n",
    "                 n_clusters=10,\n",
    "                 alpha=1.0,\n",
    "                 init='glorot_uniform'):\n",
    "\n",
    "        super(DEC, self).__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.input_dim = dims[0]\n",
    "        self.n_stacks = len(self.dims) - 1\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.autoencoder, self.encoder = autoencoder(self.dims, init=init)\n",
    "\n",
    "        # prepare DEC model\n",
    "        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(self.encoder.output)\n",
    "        self.model = Model(inputs=self.encoder.input, outputs=clustering_layer)\n",
    "\n",
    "    def pretrain(self, x, y=None, optimizer='adam', epochs=200, batch_size=256, save_dir='results/temp'):\n",
    "        print('...Pretraining...')\n",
    "        self.autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        csv_logger = callbacks.CSVLogger(save_dir + '/pretrain_log.csv')\n",
    "        cb = [csv_logger]\n",
    "        if y is not None:\n",
    "            class PrintACC(callbacks.Callback):\n",
    "                def __init__(self, x, y):\n",
    "                    self.x = x\n",
    "                    self.y = y\n",
    "                    super(PrintACC, self).__init__()\n",
    "\n",
    "                def on_epoch_end(self, epoch, logs=None):\n",
    "                    if epoch % int(epochs/10) != 0:\n",
    "                        return\n",
    "                    feature_model = Model(self.model.input,\n",
    "                                          self.model.get_layer(\n",
    "                                              'encoder_%d' % (int(len(self.model.layers) / 2) - 1)).output)\n",
    "                    features = feature_model.predict(self.x)\n",
    "                    km = KMeans(n_clusters=len(np.unique(self.y)), n_init=20, n_jobs=4)\n",
    "                    y_pred = km.fit_predict(features)\n",
    "                    # print()\n",
    "                    print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "                          % (metrics.accuracy_score(self.y, y_pred), metrics.normalized_mutual_info_score(self.y, y_pred)))\n",
    "\n",
    "            cb.append(PrintACC(x, y))\n",
    "\n",
    "        # begin pretraining\n",
    "        t0 = time.time()\n",
    "        self.autoencoder.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=cb)\n",
    "        print('Pretraining time: ', time.time() - t0)\n",
    "        self.autoencoder.save_weights(save_dir + '/ae_weights.h5')\n",
    "        print('Pretrained weights are saved to %s/ae_weights.h5' % save_dir)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_weights(self, weights):  # load weights of DEC model\n",
    "        self.model.load_weights(weights)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def predict(self, x):  # predict cluster labels using the output of clustering layer\n",
    "        q = self.model.predict(x, verbose=0)\n",
    "        return q.argmax(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def compile(self, optimizer='sgd', loss='kld'):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    def fit(self, x, y=None, maxiter=2e4, batch_size=256, tol=1e-3,\n",
    "            update_interval=140, save_dir='./results/temp'):\n",
    "\n",
    "        print('Update interval', update_interval)\n",
    "        save_interval = x.shape[0] / batch_size * 5  # 5 epochs\n",
    "        print('Save interval', save_interval)\n",
    "\n",
    "        # Step 1: initialize cluster centers using k-means\n",
    "        t1 = time.time()\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "        y_pred = kmeans.fit_predict(self.encoder.predict(x))\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "\n",
    "        # Step 2: deep clustering\n",
    "        # logging file\n",
    "        import csv\n",
    "        logfile = open(save_dir + '/dec_log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'loss'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        loss = 0\n",
    "        index = 0\n",
    "        index_array = np.arange(x.shape[0])\n",
    "        for ite in range(int(maxiter)):\n",
    "            if ite % update_interval == 0:\n",
    "                q = self.model.predict(x, verbose=0)\n",
    "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "                y_pred = q.argmax(1)\n",
    "                if y is not None:\n",
    "                    acc = np.round(metrics.accuracy_score(y, y_pred), 5)\n",
    "                    nmi = np.round(metrics.normalized_mutual_info_score(y, y_pred), 5)\n",
    "                    ari = np.round(metrics.adjusted_rand_score(y, y_pred), 5)\n",
    "                    loss = np.round(loss, 5)\n",
    "                    \n",
    "                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, loss=loss)\n",
    "                    logwriter.writerow(logdict)\n",
    "                    print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\n",
    "\n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "                y_pred_last = np.copy(y_pred)\n",
    "                if ite > 0 and delta_label < tol:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print('Reached tolerance threshold. Stopping training.')\n",
    "                    logfile.close()\n",
    "                    break\n",
    "\n",
    "            # train on batch\n",
    "            # if index == 0:\n",
    "            #     np.random.shuffle(index_array)\n",
    "            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "            self.model.train_on_batch(x=x[idx], y=p[idx])\n",
    "            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
    "\n",
    "            # save intermediate model\n",
    "            if ite % save_interval == 0:\n",
    "                print('saving model to:', save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "                self.model.save_weights(save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "\n",
    "            ite += 1\n",
    "\n",
    "        # save the trained model\n",
    "        logfile.close()\n",
    "        print('saving model to:', save_dir + '/DEC_model_final.h5')\n",
    "        self.model.save_weights(save_dir + '/DEC_model_final.h5')\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# setting the hyper parameters\n",
    "init = 'glorot_uniform'\n",
    "pretrain_optimizer = 'adam'\n",
    "dataset = 'mnist'\n",
    "batch_size = 2048\n",
    "maxiter = 2e4\n",
    "tol = 0.0001\n",
    "save_dir = 'results'\n",
    "\n",
    "import os\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "update_interval = 200\n",
    "pretrain_epochs = 500\n",
    "init = VarianceScaling(scale=1. / 3., mode='fan_in',\n",
    "                       distribution='uniform')  # [-limit, limit], limit=sqrt(1./fan_in)\n",
    "#pretrain_optimizer = SGD(lr=1, momentum=0.9)\n",
    "\n",
    "\n",
    "# prepare the DEC model\n",
    "dec = DEC(dims=[train_x.shape[-1], 500, 500, 2000, 10], n_clusters=10, init=init)\n",
    "\n",
    "dec.pretrain(x=train_x, y=train_y, optimizer=pretrain_optimizer,\n",
    "             epochs=pretrain_epochs, batch_size=batch_size,\n",
    "             save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "encoder_0 (Dense)            (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "encoder_1 (Dense)            (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "encoder_2 (Dense)            (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "encoder_3 (Dense)            (None, 10)                20010     \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) (None, 10)                100       \n",
      "=================================================================\n",
      "Total params: 1,665,110\n",
      "Trainable params: 1,665,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dec.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.compile(optimizer=SGD(0.01, 0.9), loss='kld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update interval 200\n",
      "Save interval 146.484375\n",
      "Initializing cluster centers with k-means.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: acc = 0.05088, nmi = 0.77332, ari = 0.72686  ; loss= 0\n",
      "saving model to: results/DEC_model_0.h5\n",
      "Iter 200: acc = 0.05050, nmi = 0.77894, ari = 0.73977  ; loss= 0\n",
      "Iter 400: acc = 0.05045, nmi = 0.79176, ari = 0.75790  ; loss= 0\n",
      "Iter 600: acc = 0.04940, nmi = 0.80387, ari = 0.77173  ; loss= 0\n",
      "Iter 800: acc = 0.04915, nmi = 0.81126, ari = 0.77959  ; loss= 0\n",
      "Iter 1000: acc = 0.04907, nmi = 0.81544, ari = 0.78393  ; loss= 0\n",
      "Iter 1200: acc = 0.04887, nmi = 0.81856, ari = 0.78698  ; loss= 0\n",
      "Iter 1400: acc = 0.04897, nmi = 0.82119, ari = 0.78957  ; loss= 0\n",
      "Iter 1600: acc = 0.04893, nmi = 0.82321, ari = 0.79143  ; loss= 0\n",
      "Iter 1800: acc = 0.04865, nmi = 0.82513, ari = 0.79332  ; loss= 0\n",
      "Iter 2000: acc = 0.04868, nmi = 0.82601, ari = 0.79421  ; loss= 0\n",
      "Iter 2200: acc = 0.04862, nmi = 0.82645, ari = 0.79450  ; loss= 0\n",
      "Iter 2400: acc = 0.04857, nmi = 0.82719, ari = 0.79518  ; loss= 0\n",
      "Iter 2600: acc = 0.04857, nmi = 0.82789, ari = 0.79591  ; loss= 0\n",
      "Iter 2800: acc = 0.04865, nmi = 0.82805, ari = 0.79581  ; loss= 0\n",
      "Iter 3000: acc = 0.04860, nmi = 0.82827, ari = 0.79602  ; loss= 0\n",
      "Iter 3200: acc = 0.04872, nmi = 0.82841, ari = 0.79629  ; loss= 0\n",
      "Iter 3400: acc = 0.04878, nmi = 0.82864, ari = 0.79620  ; loss= 0\n",
      "Iter 3600: acc = 0.04885, nmi = 0.82860, ari = 0.79612  ; loss= 0\n",
      "Iter 3800: acc = 0.04887, nmi = 0.82872, ari = 0.79633  ; loss= 0\n",
      "Iter 4000: acc = 0.04888, nmi = 0.82899, ari = 0.79633  ; loss= 0\n",
      "Iter 4200: acc = 0.04888, nmi = 0.82891, ari = 0.79625  ; loss= 0\n",
      "Iter 4400: acc = 0.04897, nmi = 0.82885, ari = 0.79626  ; loss= 0\n",
      "Iter 4600: acc = 0.04897, nmi = 0.82919, ari = 0.79641  ; loss= 0\n",
      "Iter 4800: acc = 0.04888, nmi = 0.82933, ari = 0.79656  ; loss= 0\n",
      "Iter 5000: acc = 0.04882, nmi = 0.82950, ari = 0.79674  ; loss= 0\n",
      "Iter 5200: acc = 0.04888, nmi = 0.82934, ari = 0.79652  ; loss= 0\n",
      "Iter 5400: acc = 0.04888, nmi = 0.82938, ari = 0.79660  ; loss= 0\n",
      "Iter 5600: acc = 0.04888, nmi = 0.82939, ari = 0.79665  ; loss= 0\n",
      "Iter 5800: acc = 0.04893, nmi = 0.82951, ari = 0.79673  ; loss= 0\n",
      "Iter 6000: acc = 0.04897, nmi = 0.82957, ari = 0.79675  ; loss= 0\n",
      "Iter 6200: acc = 0.04892, nmi = 0.82969, ari = 0.79689  ; loss= 0\n",
      "Iter 6400: acc = 0.04897, nmi = 0.82967, ari = 0.79673  ; loss= 0\n",
      "Iter 6600: acc = 0.04893, nmi = 0.82976, ari = 0.79684  ; loss= 0\n",
      "Iter 6800: acc = 0.04895, nmi = 0.82972, ari = 0.79680  ; loss= 0\n",
      "delta_label  8.333333333333333e-05 < tol  0.0001\n",
      "Reached tolerance threshold. Stopping training.\n",
      "saving model to: results/DEC_model_final.h5\n"
     ]
    }
   ],
   "source": [
    "y_pred = dec.fit(train_x, y=train_y, tol=tol, maxiter=maxiter, batch_size=batch_size,\n",
    "                 update_interval=update_interval, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = dec.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8351783283745353"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_score = normalized_mutual_info_score(test_y, pred_val)\n",
    "dec_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEpCAYAAAB8/T7dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xm8HuP9//HXW0gsiVhyaJtFUoLGUq2IqlbRVCmC2hJV9Ieglqql0i9C89XSUrT9RglVSomltNGmUvtamlBbENJQSRSx7yJ8fn9c15lObifn3CFz7hPn/Xw8zuPcM3PNPZ97m/fMXHPfo4jAzMwMYIlGF2BmZh2HQ8HMzAoOBTMzKzgUzMys4FAwM7OCQ8HMzAoOBaubpE0lPS7pdUk7Nroe6/gk7Sfp5kbXYfVzKDSQpC9JulPSK5JelHSHpI0aXVcrxgD/FxHdI+KPH/XOJF0g6aTS8DqS/iPpqI963x2ZpDUkhaRfLeR8XsGWSFoyP49v5A2V5yVdL2mXmna3S3o7t2n+u7o0fQVJv5D0VJ42XdLpklZu/0fVeA6FBpG0PPBn4FfASkBv4EfAO4t4OV0W4d2tBkz9kHUs2cb0zwE3ASdFxGkfZhmLkb2BF4HhkpZqdDEd1UK8d9eJiO7A2sDFwNmSjq1pc2DemGn+2ykvY2ngxjzvVsDywBeBV4HBi+JxLHYiwn8N+CO94V5uo83+wCPAa8DDwOfz+M8ANwMvk1bSw0rzXAD8GpgIvAEMBboBpwFPAc8CZwPL5Pa9SOH0MmlFdRuwRAu1/At4H3gLeD3f56eACXm+6cD+pfYnAleSPqSvAvu1cJ8XACcBQ4DnW2pT0/4kYDxwaa7hfmB14DhgTn58Q0vtVwB+C/wHmEXa01kiTxtICqEX87IvAnqW5p0FHAE8CLySl9ktT1slP7/Nz9mtC/G6C3gSGJmXu2Np2hrpIzlf+9uBfYD1gLeB9/Jjf770GC/Oj/9J4IeASvPvBzwKvAT8Feibxy8JBHBAfu1eAn5Zs+wD8ryvAQ8Bn83j1wFuyY//QWDb0jxN+f30KnAX8GPg5tL0QcD1+Xl7FNi5NO1iYCxwLem9u3kbz2XzY+hfM3446X26Qvk5XMB9HAg8DSzb6HVCR/lreAGd9Y+0RfICcCGwDbBizfRdgdnARnlFsgZpS32p/CH+H6ArsGX+0K6V57sgr8Q2Je0JLg2cQVp5rwT0AK4BTs7tTyaFxFL578vllUpNTU8y/0r3VuCsvIwN8oppyzztROBdYMdcxzIt3N8FwN/yCuLbdTxnJ+UP+9C8QrgEeAIYlYcPAh4vtb8m17cssCpwD7BvnrYm8NX8HK4C3AGcVpp3Vl6pfQJYGXiMHFrAqcD/5eerK7DZQrzuW+THsDwpvK8uTVtgKOTb+1FaweZxlwBX5df10/m9sXeetjMwDVgrPz8nArflac0r1D8BPYH++XUYmqePAGYCG5Lef2sCffPjfQL4QX78Q0khtUae70pSgC4LrE8K5JvztO6k9/Reefkbkj4Dze/di0nhtEl+z3Rr47lcUCgsTdqA+Vrtc9jCfVwJ/KbR64OO9NfwAjrzH2mL/4K8AppHWnGvmqdNAr7XwjxfBp6htDWfP4Qn5tsXAL8rTRNpq2v10rhNgCfy7TF5xbBGHfU+WVpp9CVttfYoTT8ZuCDfPpE2tqBzra/mlUyvOpZ/EvDX0vBOpABs3vpfMa8kupMOx71VXrEA3wauW8B97wJMLg3PAoaXhk8n9acA/IS0Il69rZoX8JivLL2W7wAr5+GFCgXSSnkesGZp3MHA9fn2deSAyMNL5uX1Lq1Qv1CafhVwVL59A3BwC/VvQVqxl/dGriDtrTXXs0Zp2s/4byh8C7ip5v5+Axybb18MnL8Qz2WLoZCnPQ/sXnoO3yTt2TT/nZCnNR+ybPj6oKP8uU+hgSLikYjYJyL6AOuSDsecmSf3JR2yqfUpYGZEvF8a92/SB73ZzNLtJtJW2z2SXpb0Mmn3vClPP5W0dfk3STMkjaqz/E8BL0bEa3XWsSBjgSnAdZJWbB4pae9Sp+A1pfbPlm6/BcwpPRdv5f/dSXtV3YBnS497LGmPAUmfkHS5pNmSXiWtrHvV1PZM6fab+X4BTsmP9QZJ/5J0dB2PE0nLkbbef59H3Z6XMaKe+VuwCtAl19Ks/BqsBowtPf7nSVvQfUrtF/QYW3v/PRV5jVqzzFVzPTNrpjVbDdi0uZ5c0+7AJ0tt6nnPtCr3E6xE2vNp9t2IWKH096M8/oWa5Xd6DoUOIiIeJa2Y1s2jZpKOl9d6Gugrqfza9SNtvRV3V7r9PGlluU7pA9EzUsccEfFaRBwZEZ8GhgFHSPpqHSU/DawkqUeddSzIe8AepP6ASbkDnoi4MP7bKbh9HfdTayZpJbdS6XEvHxHr5+k/JW01rxcRy5OO26ueO46IVyPi+xHRn3R47BhJX6lj1p1JK91xkp4hHVpZldTxDGmPDknLlub5RHnRNff3HOn5W600rvwazCQdLiuvDJeJiLvrqLWt91/5uWpe5rOk0OlbM618nzfU1NM9Ig5p5TF+GDuSXtvJdbS9HthG0jKLYLkfCw6FBpG0tqQjJfXJw31JW4x35SbnAUdJ2lDJGpJWA+4mrex+IGkpSZsD25M6YD8gb0WfC5whaZW8rN6Svp5vb5fvW6RDMe+RPtitioiZwJ3AyZKWlrQ+sC/pEMBCiYh3SX0ozwMT8xb1R5LruwU4TdLykpbIj3Oz3KQHaSX8Sn7u6z4NVtL2klZv6TmTdLGk8xYw696k12I9Uh/MBsBmwIaSPkPaan8G2FNSF0kjmX+F/yzQp/mMpfy8XQn8RFJ3SQOA7/Pf1+Bs4Nh8382nXs53umYrziO9xz6X338D8/N0J+kQ0ZH5/bcl8A3gslzPH4EfSVpG0rqkQ3bNJgDrSNojz7uUpCGS1lpQEfk03On1FCxpZUnfJp3Rd3JEvFzHbBeQnvM/SForP9Zeko5v/ox0Ng6FxnkN2Bi4W9IbpDB4CDgSICKuIJ25cUlu+0fSVu9cUghsQ1qJngXslfc0FuQY0iGiu/KhkutJnY+QzsK5ntRZ+HfgrIi4qc7HMILUQfk0cDXpOO31dc47n/y4vkk6w+aaRbTltiewHOnMrZdIx76bt7xPIJ319AppZfWHhbjftUinMb5O6qD+RUTclqf1zePmI6kfsDlwZkQ8U/r7B+n53zsfktmfdBLB86Q+hvJW/XXA46RDYs2Hfb4LzCX199xCOnHhd1C8h04Hrsiv+wNAXSu6iLiUtDd1Ganf5yrSyRDvkN5/O+QafwnsERGP51kPIvXtPEvqL/ht6T5fycvfk7SX9AypH6pbK6W0+HzWmCrpddJz8x3g0IgYU9PmbM3/PYV/5JreJp2sMZ30OrxG+iz2pL49jY8dzX9o0Mw+rHws+15g/YiY1+h6Pg4k3QAcFBGPNbqWzsKhYGZmBR8+MjOzgkPBzMwKDgUzMys4FMzMrNDqL1d2RL169Yr+/fs3ugwzs8XKPffc83xENLXVbrELhf79+zNlypRGl2FmtliR9O+2W/nwkZmZlTgUzMys4FAwM7OCQ8HMzAoOBTMzKzgUzMys4FAwM7OCQ8HMzAqL3ZfXzKxj6z/qL40u4WPryVO2rXwZ3lMwM7OCQ8HMzAoOBTMzKzgUzMysUGkoSNpa0jRJ0yWNamF6P0k3SfqnpAckfaPKeszMrHWVhYKkLsBYYBtgEDBC0qCaZscBl0fE54DhwFlV1WNmZm2rck9hCDA9ImZExFxgPLBDTZsAls+3ewJPV1iPmZm1ocrvKfQGZpaGZwEb17Q5EfibpEOB5YChFdZjiyGf816d9jjn3RY/je5oHgFcEBF9gG8AF0n6QE2SRkqaImnKnDlz2r1IM7POospQmA30LQ33yePK9gUuB4iIvwNLA71q7ygixkXE4IgY3NTU5iVGzczsQ6oyFCYDAyUNkNSV1JE8oabNU8BXASR9hhQK3hUwM2uQykIhIuYBhwCTgEdIZxlNlTRG0rDc7Ehgf0n3A5cC+0REVFWTmZm1rtIfxIuIicDEmnGjS7cfBjatsgYzM6tfozuazcysA3EomJlZwaFgZmYFh4KZmRUcCmZmVnAomJlZwaFgZmYFh4KZmRUcCmZmVnAomJlZwaFgZmYFh4KZmRUcCmZmVnAomJlZwaFgZmYFh4KZmRUqDQVJW0uaJmm6pFEtTD9D0n357zFJL1dZj5mZta6yK69J6gKMBb4GzAImS5qQr7YGQER8v9T+UOBzVdVjZmZtq3JPYQgwPSJmRMRcYDywQyvtR5Cu02xmZg1SZSj0BmaWhmflcR8gaTVgAHBjhfWYmVkbOkpH83Dgyoh4r6WJkkZKmiJpypw5c9q5NDOzzqPKUJgN9C0N98njWjKcVg4dRcS4iBgcEYObmpoWYYlmZlZWZShMBgZKGiCpK2nFP6G2kaS1gRWBv1dYi5mZ1aGyUIiIecAhwCTgEeDyiJgqaYykYaWmw4HxERFV1WJmZvWp7JRUgIiYCEysGTe6ZvjEKmswM7P6dZSOZjMz6wAcCmZmVnAomJlZwaFgZmYFh4KZmRUcCmZmVnAomJlZwaFgZmYFh4KZmRUcCmZmVnAomJlZwaFgZmYFh4KZmRUcCmZmVnAomJlZwaFgZmaFSkNB0taSpkmaLmnUAtrsJulhSVMlXVJlPWZm1rrKrrwmqQswFvgaMAuYLGlCRDxcajMQ+CGwaUS8JGmVquoxM7O2VbmnMASYHhEzImIuMB7YoabN/sDYiHgJICKeq7AeMzNrQ5Wh0BuYWRqelceVrQmsKekOSXdJ2rrCeszMrA2VHT5aiOUPBDYH+gC3SlovIl4uN5I0EhgJ0K9fv/au0cys06hyT2E20Lc03CePK5sFTIiIdyPiCeAxUkjMJyLGRcTgiBjc1NRUWcFmZp1dlaEwGRgoaYCkrsBwYEJNmz+S9hKQ1It0OGlGhTWZmVkrKguFiJgHHAJMAh4BLo+IqZLGSBqWm00CXpD0MHATcHREvFBVTWZm1rpK+xQiYiIwsWbc6NLtAI7If2Zm1mD+RrOZmRUcCmZmVnAomJlZwaFgZmYFh4KZmRUcCmZmVnAomJlZwaFgZmYFh4KZmRUcCmZmVnAomJlZwaFgZmYFh4KZmRUcCmZmVnAomJlZwaFgZmaFSkNB0taSpkmaLmlUC9P3kTRH0n35b78q6zEzs9ZVduU1SV2AscDXgFnAZEkTIuLhmqaXRcQhVdVhZmb1q3JPYQgwPSJmRMRcYDywQ4XLMzOzj6jKUOgNzCwNz8rjau0s6QFJV0rqW2E9ZmbWhsoOH9XpGuDSiHhH0gHAhcCWtY0kjQRGAvTr1+9DL6z/qL986HmtdU+esm2jSzCzRaCuPQVJu0rqkW8fJ+kqSZ9vY7bZQHnLv08eV4iIFyLinTx4HrBhS3cUEeMiYnBEDG5qaqqnZDMz+xDqPXx0fES8JulLwFDgN8Cv25hnMjBQ0gBJXYHhwIRyA0mfLA0OAx6psx4zM6tAvaHwXv6/LTAuIv4CdG1thoiYBxwCTCKt7C+PiKmSxkgalpsdJmmqpPuBw4B9FvYBmJnZolNvn8JsSeeQTi/9qaRu1BEoETERmFgzbnTp9g+BH9ZfrpmZVanePYXdSFv8X4+Il4GVgKMrq8rMzBqirlCIiDeB54Av5VHzgMerKsrMzBqj3rOPTgCO4b+HepYCLq6qKDMza4x6Dx/tRDo76A2AiHga6FFVUWZm1hj1hsLciAggACQtV11JZmbWKPWGwuX57KMVJO0PXA+cW11ZZmbWCHWdkhoRp0n6GvAqsBYwOiKuq7QyMzNrd22GQv4J7OsjYgvAQWBm9jFWzxfQ3gPel9SzHeoxM7MGqvcbza8DD0q6jnwGEkBEHFZJVWZm1hD1hsJV+c/MzD7G6u1ovjD/0umaedS0iHi3urLMzKwR6goFSZuTLoDzJCCgr6S9I+LW6kozM7P2Vu/ho58DW0XENABJawKXsoCL4piZ2eKp3i+vLdUcCAAR8Rjp94/MzOxjpN5QmCLpPEmb579zgSltzSRpa0nTJE2XNKqVdjtLCkmD6y3czMwWvXpD4SDgYdLV0Q7Ltw9qbYb8pbexwDbAIGCEpEEttOsBfA+4u/6yzcysCvX2KSwJ/CIiTodihd+tjXmGANMjYkaeZzywAylQyv4X+Cm+aI+ZWcPVu6dwA7BMaXgZ0o/itaY3MLM0PCuPK0j6PNA3X/PZzMwarN5QWDoiXm8eyLeX/SgLlrQEcDpwZB1tR0qaImnKnDlzPspizcysFfWGwht5qx6A3CH8VhvzzAb6lob75HHNegDrAjdLehL4AjChpc7miBgXEYMjYnBTU1OdJZuZ2cKqt0/hcOAKSU/n4U8Cu7cxz2RgoKQBpDAYDuzRPDEiXgF6NQ9Luhk4KiLaPKvJzMyq0eqegqSNJH0iIiYDawOXAe8C1wJPtDZvRMwDDgEmAY8Al0fEVEljJA1bJNWbmdki1daewjnA0Hx7E+B/gEOBDYBxwC6tzRwRE4GJNeNGL6Dt5m2Xa2ZmVWorFLpExIv59u7AuIj4A/AHSfdVW5qZmbW3tjqau0hqDo6vAjeWptXbH2FmZouJtlbslwK3SHqedLbRbQCS1gBeqbg2MzNrZ62GQkT8WNINpLON/hYRkSctQepbMDOzj5E2DwFFxF0tjHusmnLMzKyR6v3ympmZdQIOBTMzKzgUzMys4FAwM7OCQ8HMzAoOBTMzKzgUzMys4FAwM7OCQ8HMzAoOBTMzKzgUzMysUGkoSNpa0jRJ0yWNamH6gZIelHSfpNslDaqyHjMza11loSCpCzAW2AYYBIxoYaV/SUSsFxEbAD8DTq+qHjMza1uVewpDgOkRMSMi5gLjgR3KDSLi1dLgckBgZmYNU+XV03oDM0vDs4CNaxtJOhg4AugKbFlhPWZm1oaGdzRHxNiIWB04BjiupTaSRkqaImnKnDlz2rdAM7NOpMpQmA30LQ33yeMWZDywY0sTImJcRAyOiMFNTU2LsEQzMyurMhQmAwMlDZDUFRgOTCg3kDSwNLgt8HiF9ZiZWRsq61OIiHmSDgEmAV2A8yNiqqQxwJSImAAcImko8C7wErB3VfWYmVnbquxoJiImAhNrxo0u3f5elcs3M7OF0/COZjMz6zgcCmZmVnAomJlZwaFgZmYFh4KZmRUcCmZmVnAomJlZwaFgZmYFh4KZmRUcCmZmVnAomJlZwaFgZmYFh4KZmRUcCmZmVnAomJlZwaFgZmaFSkNB0taSpkmaLmlUC9OPkPSwpAck3SBptSrrMTOz1lUWCpK6AGOBbYBBwAhJg2qa/RMYHBHrA1cCP6uqHjMza1uVewpDgOkRMSMi5gLjgR3KDSLipoh4Mw/eBfSpsB4zM2tDlaHQG5hZGp6Vxy3IvsBfK6zHzMzasGSjCwCQtCcwGPjKAqaPBEYC9OvXrx0rMzPrXKrcU5gN9C0N98nj5iNpKHAsMCwi3mnpjiJiXEQMjojBTU1NlRRrZmbVhsJkYKCkAZK6AsOBCeUGkj4HnEMKhOcqrMXMzOpQWShExDzgEGAS8AhweURMlTRG0rDc7FSgO3CFpPskTVjA3ZmZWTuotE8hIiYCE2vGjS7dHlrl8s3MbOH4G81mZlZwKJiZWcGhYGZmBYeCmZkVHApmZlZwKJiZWcGhYGZmBYeCmZkVHApmZlZwKJiZWcGhYGZmBYeCmZkVHApmZlZwKJiZWcGhYGZmBYeCmZkVKg0FSVtLmiZpuqRRLUzfTNK9kuZJ2qXKWszMrG2VhYKkLsBYYBtgEDBC0qCaZk8B+wCXVFWHmZnVr8rLcQ4BpkfEDABJ44EdgIebG0TEk3na+xXWYWZmdary8FFvYGZpeFYeZ2ZmHdRi0dEsaaSkKZKmzJkzp9HlmJl9bFUZCrOBvqXhPnncQouIcRExOCIGNzU1LZLizMzsg6oMhcnAQEkDJHUFhgMTKlyemZl9RJWFQkTMAw4BJgGPAJdHxFRJYyQNA5C0kaRZwK7AOZKmVlWPmZm1rcqzj4iIicDEmnGjS7cnkw4rmZlZB7BYdDSbmVn7cCiYmVnBoWBmZgWHgpmZFRwKZmZWcCiYmVnBoWBmZgWHgpmZFRwKZmZWcCiYmVnBoWBmZgWHgpmZFRwKZmZWcCiYmVnBoWBmZgWHgpmZFSoNBUlbS5omabqkUS1M7ybpsjz9bkn9q6zHzMxaV1koSOoCjAW2AQYBIyQNqmm2L/BSRKwBnAH8tKp6zMysbVXuKQwBpkfEjIiYC4wHdqhpswNwYb59JfBVSaqwJjMza0WVodAbmFkanpXHtdgmIuYBrwArV1iTmZm1YslGF1APSSOBkXnwdUnTGllPO+oFPN/oIuohH/iDxej1Ar9mWWd6zVarp1GVoTAb6Fsa7pPHtdRmlqQlgZ7AC7V3FBHjgHEV1dlhSZoSEYMbXYfVx6/X4sev2QdVefhoMjBQ0gBJXYHhwISaNhOAvfPtXYAbIyIqrMnMzFpR2Z5CRMyTdAgwCegCnB8RUyWNAaZExATgN8BFkqYDL5KCw8zMGkTeMO+4JI3Mh85sMeDXa/Hj1+yDHApmZlbwz1yYmVnBoWBmViJpOUk9Gl1HozgUPibyz4pYB+Nv6C9eJB0B/A3YVdKnG11PIywWX16ztkXEe5KWAvYD7o6IextdU2cmSZE1uhZrXQ7u3sBFwLOkz9AzwJuNrKtRvKewmGreAi39/yZwN7Au8J8GlmZAcxhI2l7S+fn/anmcP3cdhKSl8mu1JnBfRAyPiEci4qWIeKfR9TWC35yLIUlLNK90SluiQ4HTI+Jg4HlJPRtWYCekZPmacaOA44B/Al8HfgEQEe+3f4VWJmlZSacCw/KobYBl8rRuDSusA3AoLEaa9woi4n1JfSQdKOlLefLjwLGSzgEuAS6UtHujau2EtiP9ym9/SbtI6k7a+twrIn4FHE16CQ8C9zU0kqSDgeuAFUn9BwB3ACtK6hkR75RfH0mrNqDMhnGfwmKkdEhiZ+CHwD3ASZK2iogzJD0JPEJ6Xb8A9GtUrZ1Fc98B6Te7zgO6AtdExJWSBpK2QKdFxFuS/gR8Nu/peW+hneXfVzsKOAFYJyJm5PErk/oQngO2By4mfYbezeG+iaRrI+LtxlTevryn0IFJWqJmi2XJfHbE4cDxEXEA8HPghHxs9OqIeBRYHTgYeKwhhXciERGS1gbeAv4F3AIcnyefCHxX0gp5eHXgMQdC+2o+My//PP9NwDXAXEkrSvoz6eJejwIPAbtJGhoR70pqAn4LbA50mtfModBB5S3Q9/NKZ11Jn85v6htIvxM1ECAiTgZWIp1C10PSaOAHwKER8aeGPYBOQtIA4EhgU+BbgIDtJC0fETcAdwLnSroG2Az4R8OK7WQkdcm/tXaKpJGS1ouIu4G7SOFwPXB9ROwXES+TDrtOBH4i6WLSIaV7I+LwfKGwTsE/c9GB5Y7LM4H1SW/guRExWtKhwCeBSyLiIUk7AT8DNgGWigiffbSINR/yKe257RERv5e0LPANUoflEaTDRZsAp0TEU5J6ASsAG0bEZQ0pvhOStC/wHWAacCOwNfAV0hUhuwDnA3/O/T218/YgfeYeiYgX263oDsKh0EHUHmfOZ0DsCSwbEb/KZ0psBxwL3Jr/TwPOyXsTewEX+bz4Ra/Ub9C8wngHeBvYNCL+LmkN0k/Av0E6nHcm0IN0RtiB+ReBrZ1IWoXURzAoH05tHn8B0DMidpI0HNgdGBERb0vajNSfcF5EdJaLeLXIodDBSNqSdCbRM6RDEasC5wCvk/YWdiVtlX4T2IJ0GurDjam285D0ReA0YAbpS04Dgb0jYqN8zHpP0k+/H016/YYBT+XDFdbOJJ0LTI6IcZKWjYg3JS1DuvzvMGAK6fWcl2f5EnBGRFzamIo7DvcpdBCS+km6irQHsB9wYT6OuT7pSzW7AfcCA4BRwGXATx0I1ZO0HXAycChwFvB70mvRJGmviHiP1M/TDRgeEe9ExBUOhIY6nNSXsHQOhG4R8Rap43hE/mz9BdgNeD0ihjgQEu8pNICkLnlFUh43AngXuAo4FdgRWA84DPgisC9wDOmr9/dFxFXtWnQnpnTlwOWAbUkrm9dJVxZ8PA/fRjpWPToirmlUnTY/SQcCG0fEdyR1jYi5kn4H3BURZ+X+oCUj4tUGl9qhOBTaUfnYdB7eHLgl9wlcBMwlfeHpQWBURLyq9HtGPwc2BqYCB3XWr983kqTVgbGkDuYXJb1A6lieTXrN/hIR/25kjTa//HMiTwFfjognJG0A/Jh0Ord/G2wB/OW1dpDPWFFzR7KkvsCvScelJ+bjn5cAfwZWi4hZud1ewD8i4jBJK0bES415BEbq3+kBdM+dzf8ifTnwioi4vqGVWYvy2WK7AX+QNJHUkfxLB0LrHAoVK+0dRN7a3AxYHrgQuJp0jvsBpN/IuQk4SNJNpMNFnyIdpsCB0HBPk85hv4r0uflRRFzd2JKsLRFxp6RXSJ+5Id7LbpsPH7WDvKfwHdLKfwrwbWBkRIyXtFEevhm4PbfbGLgjIn7emIptQfLZYXd45bL4aKkPzxbMobCISdoKWLn5TIbcgbwC6WcnjoqIa/N3DvpHxK65E3M30q7t8RHxWP7Jincb9RjMrPPyKamLXk9gdF7ZA+wBPEHqPG7+RdNRwCBJ2+dT4+4lfevyNQAHgpk1ikNh0buS1A9wnKR+QLeIuJbUf9BL0kZ5V/ZM4JcAEfFwRJzjn6cws0ZzR/Milk8v/TnpW689839InchrADuRvml5rqT34IOnqpqZNYr7FCoi6Vjgf0kX85gB/BXoS7pc5vkRMbmB5ZmZtcihUJF8tabxwNnAS8BWpFNMj4qIpxtZm5nZgjgUKqR06cUDImKDRtdiZlYP9ylU63xgXv66vXyutJl1dN5TMDOzgk9JNTOzgkPBzMwKDgUzMys4FMzMrOBQMDOzgkPBOgVJn5A0XtK/JN0jaaKkNRfQdgVJ322nug7MF1My6xB8Sqp97OXrWdwJXBgRZ+dxnwWWj4jbWmjfH/hzRKxbcV1LRsS8KpdhtrC8p2AsmIKdAAAChUlEQVSdwRbAu82BABAR9wP/lHSDpHslPShphzz5FGB1Sffla18g6WhJkyU9IOlHzfcj6XhJ0yTdLulSSUfl8RtIuiu3v1rSinn8zZLOlDQF+J6kE0vzrC7p2rwnc5uktfP4XSU9JOl+Sbe2w/NlnZi/0WydwbrAPS2MfxvYKSJeldQLuEvSBNL1LtZt/nmSfOGkgcAQ0rWaJ0jaDHgL2Bn4LLAU6boYzcv5HXBoRNwiaQxwAnB4ntY1Igbn+z6xVM844MCIeFzSxsBZwJbAaODrETFb0gof/ekwWzCHgnVmAn6SV/DvA72BVVtot1X++2ce7k4KiR7AnyLibeBtSdcASOoJrBARt+T2FwJXlO7vsg8UInUHvghckY52AdAt/78DuEDS5aRrRJtVxqFgncFUYJcWxn8LaAI2jIh3JT0JLN1COwEnR8Q5842UDm+hbT3eaGHcEsDLLf14YkQcmPcctgXukbRhRLzwIZdt1ir3KVhncCPQTdLI5hGS1gdWA57LgbBFHoZ0WdQepfknAf8vb80jqbekVUhb8NtLWjpP2w4gIl4BXpL05Tz/t4FbaEVEvAo8IWnXvAzlznAkrR4Rd0fEaGAO6bocZpXwnoJ97OWr4e0EnCnpGFJfwpPAicAvJT0ITAEeze1fkHSHpIeAv0bE0ZI+A/w9H9p5HdgzIibnPogHgGdJ1+F+JS92b+BsScuSLrL0nTpK/Rbwa0nHkfooxgP3A6dKGkjaY7khjzOrhE9JNfsIJHWPiNfzyv9WYGRE3Nvousw+LO8pmH004yQNIvVFXOhAsMWd9xTMzKzgjmYzMys4FMzMrOBQMDOzgkPBzMwKDgUzMys4FMzMrPD/AcDHWSVsgHqsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Need to plot the accuracies and save the results\n",
    "# kmeans_score = 0.49\n",
    "# auto_score = 0.8\n",
    "# dec_score = 0.835\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.bar([0,1,2], [kmeans_score, auto_score, dec_score])\n",
    "plt.xticks([0,1,2], ['kmeans', 'auto', 'DEC'], rotation=30)\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Scores for K-means, Autoencoder, DEC')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "savefig('scores_for_prototype.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
